# -*- coding: utf-8 -*-
"""Sahaay MobileNet V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dej7EpbwGQUuS-DspBDx2jiR2b5_dOit
"""

! git clone https://github.com/Afeefaa/AWS

from tensorflow import keras
from keras.layers import Dropout,Conv2D
from keras.layers import MaxPool2D
from keras.layers import ReLU, Dense, Flatten
from keras.layers import SpatialDropout2D,AveragePooling2D,BatchNormalization, GlobalAveragePooling2D

import numpy as np
import os
import matplotlib.pyplot as plt
import cv2
from keras.preprocessing.image import ImageDataGenerator

import pathlib
import PIL
import tensorflow as tf
from tensorflow.keras import layers, regularizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

!pip install keras_cv

os.chdir('/content/AWS')

batchsize=35

train_datagen= ImageDataGenerator(
  rotation_range = 40,
  shear_range=0.1,
  height_shift_range=0.2,
  zoom_range=0.2,
  horizontal_flip=True,
  vertical_flip=True,
  width_shift_range=0.2,
)

train_generator=train_datagen.flow_from_directory(
    'train',
    target_size=(256,256),
    batch_size=batchsize,
    class_mode='categorical'
)

train_generator

os.chdir('predb')

y_val=np.array([]).reshape(0,6)
x_val=np.array([]).reshape(0,256,256,3)


def encode_class(x):
  l= [0 for i in range(6)]
  l[x]= 1
  return np.array(l).reshape(1,6)

def load_data(name):
  global x_val
  global y_val

  x=5

  global image
  image=plt.imread(name)
  image=cv2.resize(image,(256,256))
  image=image.reshape((1,256,256,3))

  x_val=np.append(x_val,image,axis=0)

  if 'cardboard' in name:
    x=0
  elif 'glass' in name:
    x=1
  elif 'metal' in name:
    x=2
  elif 'paper' in name:
    x=3
  elif 'plastic' in name:
    x=4

  y_val=np.append(y_val,encode_class(x), axis=0)

for i in os.listdir()[1:]:
  load_data(i)

# print(image[0])
# print(image.shape)

# print(x_val.shape,'\t',y_val[0])
os.chdir("..")

input_shape = (256, 256, 3)

pre_inc = tf.keras.applications.MobileNetV2(
    include_top=False,
    weights="imagenet",
    input_shape= input_shape,
    pooling=None,
    classes=1000,
    classifier_activation="softmax",
)


for i in pre_inc.layers[:-10]:
  i.trainable = False

x = layers.GlobalAveragePooling2D()(pre_inc.output)
x = layers.Dropout(0.5)(x)
x = layers.Dense(512, activation='relu')(x)
x = layers.Flatten()(x)
model_layers = layers.Dropout(0.5)(x)
model_layers = layers.BatchNormalization()(model_layers)
model_layers = layers.GaussianNoise(0.25)(model_layers)
model_layers = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(model_layers)
model_layers = layers.Dropout(0.5)(model_layers)
model_layers = layers.BatchNormalization()(model_layers)
model_layers = layers.Dense(128, activation='relu')(model_layers)
model_layers = layers.Dropout(0.5)(model_layers)
model_layers = layers.BatchNormalization()(model_layers)


combined_layers = layers.concatenate([x, model_layers])
x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(combined_layers)
combined_layers = layers.Dropout(0.5)(combined_layers)
pred = layers.Dense(6, activation='softmax')(combined_layers)
model = keras.Model(inputs=pre_inc.input, outputs=pred)

model.summary()

model.compile(optimizer=Adam(learning_rate=0.0000050*batchsize),loss='categorical_crossentropy',metrics=['accuracy'])
# print(model.output_shape)

# CHECKPOINT:
checkpoint = keras. callbacks.ModelCheckpoint('ckpt.keras', monitor = 'val_accuracy', verbose = 1, save_best_only = True, mode = 'max')

# EARLY STOPPING:

stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)

try:
  model.load_weights('ckpt.keras')
except:

  print('No Pre- existing Checkpoint, creating new one')


from keras.callbacks import ReduceLROnPlateau

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_learning_rate=1e-6)

history = model.fit(
    train_generator, steps_per_epoch=1995//batchsize, epochs=30,validation_data=(x_val,y_val),callbacks=[checkpoint,stop]
)
history
model.save("AWS")

num_layers = len(pre_inc.layers)
print(num_layers)

for layer in pre_inc.layers[:-20]:
    layer.trainable = True

optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

history_fine_tune = model.fit(
    train_generator,
    steps_per_epoch=1995 / 35,
    epochs = 30,
    validation_data=(x_val,y_val),
    callbacks=[checkpoint,stop],
)

log_preds = model.predict(x_val)
preds = np.argmax(log_preds, axis=1)

model.save_weights("modelcheckpoint",save_format="tf")

from sklearn.metrics import confusion_matrix
import seaborn as sns
cm = confusion_matrix(y_val.argmax(axis=1), preds)

class_names = train_generator.class_indices.keys()
plt.figure(figsize=(len(class_names), len(class_names)))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()